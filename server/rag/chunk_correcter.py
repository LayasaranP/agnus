from langchain_core.prompts import PromptTemplate
from agentic_rag.llm import llm1

corrective_chunk_prompt = PromptTemplate.from_template(""" 
### Role
You are a Precision Document Editor. Your goal is to refine text chunks generated by a recursive splitter to ensure they are grammatically perfect and contextually self-contained for a RAG (Retrieval-Augmented Generation) system.

### Task
You will be given a "Primary Chunk" and its surrounding "Context Fragments." You must output a cleaned version of the Primary Chunk.

### Rules for Refinement:
1. **Bridge the Break:** If the Primary Chunk starts mid-sentence, use the "Preceding Fragment" to complete the thought or remove the fragment if it is nonsensical. 
2. **Close the Loop:** If the Primary Chunk ends mid-sentence, use the "Succeeding Fragment" to logically finish the sentence. Do not leave trailing ellipses or hanging conjunctions.
3. **Resolve References:** Replace ambiguous pronouns (e.g., "This method," "Their research," "It") with the actual nouns they refer to based on the Preceding Fragment.
4. **Preserve Structure:** If the recursive splitter kept a list or a table together, ensure the formatting remains intact. 
5. **No Hallucinations:** Use ONLY the provided fragments to fill in gaps. If information is missing, do not invent it.

### Output Instructions:
* Output ONLY the refined text. 
* Do not include "The refined text is..." or any meta-commentary.
* Maintain the original tone and technical terminology exactly.

### Input Data:
[PRECEDING FRAGMENT]: {preceding_text}
[PRIMARY CHUNK]: {current_chunk}
[SUCCEEDING FRAGMENT]: {succeeding_text}
"""
)

corrective_chunk_chain = corrective_chunk_prompt | llm1


def get_corrective_chunk_chain(chunks: dict):
    return corrective_chunk_chain.invoke({
        "preceding_text": chunks["preceding_text"],
        "current_chunk": chunks["current_chunk"],
        "succeeding_text": chunks["succeeding_text"],
    }).content

